
# This is an example config for configuring KataGo to attempt to imitate a weaker human player.
# Running with this config requires giving a human SL model b18c384nbt-humanv0.bin.gz
# on the command line such as:
# ./katago gtp -config gtp_human5k_example.cfg -model your_favorite_normal_model_for_katago.bin.gz -human-model b18c384nbt-humanv0.bin.gz
# You can obtain the human model at https://github.com/lightvector/KataGo/releases/tag/v1.15.0

# Below, the most important parts of the config for human-like play are commented.
# See the original gtp_example for comments on other parameters.

# For another useful guide on human-style analysis, see here:
# https://github.com/lightvector/KataGo/blob/master/docs/Analysis_Engine.md#human-sl-analysis-guide

# It is ALSO possible to pass in simply '-model b18c384nbt-humanv0.bin.gz' and NOT
# pass in -human-model, i.e. use the human model as if it were KataGo's normal neural net.
# If you do that, you need to use a config more like the normal gtp_example.cfg, not this config!
# Keep in mind that if using the model normally, or if using -human-model but also altering
# parameters below to blend in some of KataGo's search, KataGo's play might NOT be very human-like,
# or might be human-like but play at a strength very different than the humanSLProfile.
# You can experiment, some of the comments below hopefully will help illustrate things too.

logDir = gtp_logs
logAllGTPCommunication = true
logSearchInfo = true
logSearchInfoForChosenMove = false
logToStderr = false

# Use these rules by default, but a GUI or GTP controller might override this.
rules = japanese

# When using -human-model, we only resign when far behind since a weaker player
# might continue to fight much longer than a strong bot normally would.
allowResignation = true
resignThreshold = -0.99
resignConsecTurns = 20
resignMinScoreDifference = 40
resignMinMovesPerBoardArea = 0.4

# Note: unless you change other parameters too, by default increasing visits won't do much.
# If humanSLChosenMoveProp = 1.0 AND humanSLChosenMovePiklLambda is a large number,
# then KataGo's normal search is ignored except for possibly choosing whether to pass/resign,
# so more visits will have no effect on play. Still, having some visits is good for
# ensuring good pass/resign judgment.
maxVisits = 40
numSearchThreads = 1
lagBuffer = 1.0

# Rough scale in seconds to randomly delay moving, so as not to respond instantly.
# Some moves will delay longer, some moves will delay a little less.
delayMoveScale = 2.0
delayMoveMax = 10.0

# ===========================================================================
# HUMAN SL PARAMETERS
# ===========================================================================

# The most important parameter for human-like play configuration!
# Choose the "profile" of players that the human SL model will imitate.
# Available options are:
# preaz_{RANK from 20k to 9d} - imitate player of given rank, before AlphaZero opening style became popular
# rank_{RANK from 20k to 9d} - imitate player of given rank, after human openings changed due to AlphaZero.
# preaz_{BRANK}_{WRANK} or rank_{BRANK}_{WRANK} - same, but imitate how black with the rank BR and white
#   with the rank WR would play against each other, knowing that the other player is stronger/weaker than them.
#   Warning: for rank differences > 9 ranks, or drastically mis-matched to the handicap used in the game,
#   this may be out of distribution due to lack of training data and the model might not behave well! Experiment with care.
# proyear_{YEAR from 1800 to 2023} - imitate historical pros or insei from given year.
humanSLProfile = preaz_5k

# The probability that we should play a HUMAN-like move, rather than playing KataGo's move.
# Applies BEFORE temperature.
humanSLChosenMoveProp = 1.0

# If true, ignore the human SL model's choice of when to pass, and still use KataGo to determine that.
# The human SL model, in theory, is not guaranteed to be reliable at when to pass for all profiles,
# since e.g. some historical games omit passes.
humanSLChosenMoveIgnorePass = true

# By default humanSLChosenMovePiklLambda is a large number which effectively disables it.
# Setting it to a smaller number will "suppress" human-like moves that KataGo disapproves of.
# In particular, if set to, for example, 0.4 when KataGo judges a human SL move to lose 0.4 utility,
# it will substantially suppress the chance of playing that move (in particular, by a factor of exp(1)).
# Less-bad moves will also be suppressed, but not by as much, e.g. a move losing 0.2 would get lowered
# by a factor of exp(0.5).
# As configured lower down, utilities by default range from -1.0 (loss) to +1.0 (win), plus up to +/- 0.3 for score.
# WARNING: ONLY moves that KataGo actually searches will get suppressed! If a move is so bad that KataGo
# rejects it without searching it, it will NOT get suppressed.
# Therefore, to use humanSLChosenMovePiklLambda, it is STRONGLY recommended that you also use something
# like humanSLRootExploreProbWeightless to ensure most human moves including bad moves get searched,
# and ALSO use at least hundreds and ideally thousands of maxVisits, to ensure enough visits.
humanSLChosenMovePiklLambda = 100000000

# These parameters tell KataGo to use the human SL policy for exploration during search.
# Each of these specifies the probability that KataGo will perform PUCT using the Human SL policy to
# explore different moves, rather than using KataGo's normal policy, after a certain minimal number of visits.
# "Root": applies only at the root of the search
# "Pla": applies during non-root nodes of the search where it is katago's turn.
# "Opp": applies during non-root nodes of the search where it is the opponent's turn.
# "Weightless": search the move to evaluate it, but do NOT allow this visit to affect the parent's average utility.
# "Weightful": search the move to evaluate it, and DO allow this visit to affect the parent's average utility.
# For example, humanSLRootExploreProbWeightless = 0.5 would tell KataGo at the root of the search to spend
# 50% of its visits to judge different possible human moves, but NOT to use those visits for determining the
# value of the position (avoiding biasing the utility if some human SL moves are very bad).
# If you don't understand these well, ask for help or look up some online explainers for MCTS (Monte-Carlo Tree Search).
humanSLRootExploreProbWeightless = 0.0
humanSLRootExploreProbWeightful = 0.0
humanSLPlaExploreProbWeightless = 0.0
humanSLPlaExploreProbWeightful = 0.0
humanSLOppExploreProbWeightless = 0.0
humanSLOppExploreProbWeightful = 0.0

# When using the human SL policy for exploration during search, use this cPUCT.
# This only has an effect if at least one of humanSL{Root,Pla,Opp}ExploreProbWeight{less,ful} is nonzero.
humanSLCpuctExploration = 0.50

# Same as humanSLCpuctExploration, but NEVER diminshes its exploration no matter how many visits are used.
# Normally, PUCT will sharpen with visits and spend a diminishing proportion of visits on moves with lower utility.
# This is the coefficient for a term that does NOT diminish, i.e. if this is 0.2, then roughly moves within
# 0.2 utility (about 10% winrate) of the best move will forever continue getting a decent fraction of visits,
# smoothly falling off for greater utility differences.
# Note that in combination with Weightful exploration above, if used for Opp exploration, this could be used
# to model an opponent that will always have some chance to make small mistakes no matter how deep they search.
# If further this was increased to a very large value, it would model an opponent that always played according
# to the human SL raw policy. These might be interesting to experiment with for handicap play.
humanSLCpuctPermanent = 0.2


# ===========================================================================
# OTHER USEFUL PARAMETERS FOR HUMAN PLAY ADJUSTMENT
# ===========================================================================

# Choosing temperature near 1, and restricting it to only affect moves already below 1% chance,
# so that we sample close to the full range of human play.
# You can also reduce the temperature to settings more like the plain gtp_example.cfg.
# Then, rather than imitating a realistic human player, it will be more like imitating the
# *majority vote* of players at that rank. For example it would avoid a lot of blunders
# that players of that level would make, because even if players often blunder, the *majority vote*
# of players would be much less likely to select any given blunder that an individual player would.
chosenMoveTemperatureEarly = 0.85
chosenMoveTemperature = 0.70
chosenMoveTemperatureHalflife = 80
chosenMoveTemperatureOnlyBelowProb = 0.01  # temperature only starts to dampen moves below this
chosenMoveSubtract = 0
chosenMovePrune = 0

# Use a small NN cache to save memory since we're using very low visits anyways. You can increase
# these back to more like the plain gtp_example.cfg if you are doing more extensive searches to
# improve performance.
nnCacheSizePowerOfTwo = 17
nnMutexPoolSizePowerOfTwo = 14

# ===========================================================================
# PARAMETERS CHANGED FROM DEFAULT TO MAKE SURE HUMAN SL USAGE WORKS WELL
# ===========================================================================

# Make sure to take into account the recent moves in the game, don't ignore history.
# This will produce the best imitation/prediction, since humans definitely do play differently based on where the
# most recent moves in the game were, rather than coming fresh to the board position on every turn.
ignorePreRootHistory = false
analysisIgnorePreRootHistory = false

# Average 2 neural net samples at the root - ensures a bit smoother probabilities and results in
# 8 * 7 / 2 = 28 possible policies instead of 8 possibilities.
rootNumSymmetriesToSample = 2
# LCB improves strength for KataGo, but we disable it so it doesn't mess up move selection when blending human play.
useLcbForSelection = false

# We disable dynamicScoreUtilityFactor - the human SL model can make score predictions that are a bit swingy, so
# if we do want to do a search that blends human SL values in (TODO there isn't a way to do this anyways yet), using
# static score utility might be a bit more stable.
winLossUtilityFactor = 1.0
staticScoreUtilityFactor = 0.30
dynamicScoreUtilityFactor = 0.00

# Uncertainty improves strength for KataGo normally, but messes with the weights of playouts in complicated ways,
# so lets turn it off when doing human SL stuff.
useUncertainty = false

# Subtree value bias improves strength for KataGo normally, but messes with the values of nodes in complicated ways,
# so let's turn it off when doing human SL stuff.
subtreeValueBiasFactor = 0.0

# Noise pruning prunes out weight from moves that KataGo thinks are bad, but if we are doing human SL we might actively
# want to be playing or exploring and weighting "bad" but human-like moves. So disable this.
# Warning: when this is false, there is much less protection against the search severely misbehaving when you use too many threads.
# Make sure not to set numSearchThreads to be too large - at a minimum, keep at least a 20x buffer between
# the number of visits you use and the number of threads you use.
# (as an aside, ideally, you want to have visits be a sufficient factor larger than threads EVEN when
# useNoisePruning is true, this parameter just blunts the worst effects but doesn't entirely fix the badness).
useNoisePruning = false

# CoreML settings--------------------------------------

# IF USING ONE MODEL:
numNNServerThreadsPerModel = 1
coremlDeviceToUse = 0 # GPU
# coremlDeviceToUse = 100 # Neural Engine

# IF USING TWO MODEL: Uncomment these three lines
# numNNServerThreadsPerModel = 2
# coremlDeviceToUseThread0 = 0 # GPU
# coremlDeviceToUseThread1 = 100 # Neural Engine
