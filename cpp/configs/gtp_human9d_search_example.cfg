
# This is an example config for configuring KataGo to attempt to imitate a 9d player and then
# further improving the strength using KataGo's evaluations and search.

# By contrast, gtp_human5k_example.cfg uses only the raw neural net. Raw neural nets like the
# HumanSL net without search do NOT play accurately enough to reach top human levels.
# If you were to take gtp_human5k_example.cfg and swap out preaz_5k -> preaz_9d,
# despite *attempting* to imitate a 9d player, it would not actually achieve that strength.

# Due to the use of search, this config plays significantly stronger, and depending on how
# you adjust visits and parameters and what `-model` you use for KataGo's model, can probably
# reach 9d strength or higher.

# Using the search greatly reduces the "humanness" of the play in exchange for strength,
# but still retains noticeable bias from the human SL policy and may still play somewhat
# "human-style" openings and moves.

# Running with this config requires giving a human SL model b18c384nbt-humanv0.bin.gz
# on the command line such as:
# ./katago gtp -config gtp_human5k_example.cfg -model your_favorite_normal_model_for_katago.bin.gz -human-model b18c384nbt-humanv0.bin.gz
# You can obtain the human model at https://github.com/lightvector/KataGo/releases/tag/v1.15.0

# You can compare the config parameters here with gtp_human5k_example.cfg and see the differences
# and how they were achieved.
# gtp_human5k_example.cfg also has a bit more explanation on many of the parameters.

logDir = gtp_logs
logAllGTPCommunication = true
logSearchInfo = true
logSearchInfoForChosenMove = false
logToStderr = false

rules = japanese

# Adjusted resignation settings compared to gtp_human5k_example.cfg, to resign a bit more easily.
allowResignation = true
resignThreshold = -0.98  # was -0.99 in gtp_human5k_example.cfg
resignConsecTurns = 10   # was 20 in gtp_human5k_example.cfg
resignMinScoreDifference = 20      # was 40 in gtp_human5k_example.cfg
resignMinMovesPerBoardArea = 0.40

# maxVisits: With a config like this, more visits *will* make it stronger and fewer visits will make it weaker.
# But strength will NOT scale indefinitely upward with visits unlike KataGo's normal config without human SL.
# This is due to the strong bias of the human SL network.
# You can reduce this number if you are on weaker hardware. It may reduce strength a bit but will still
# provide a huge strength boost over using the humanSL network alone as in gtp_human5k_example.cfg
maxVisits = 800   # 40 in gtp_human5k_example.cfg.

# Having more than one thread speeds up search when visits are larger.
numSearchThreads = 4  # 1 in gtp_human5k_example.cfg.
lagBuffer = 1.0

# Rough scale in seconds to randomly delay moving, so as not to respond instantly.
# Some moves will delay longer, some moves will delay a little less.
delayMoveScale = 2.0
delayMoveMax = 10.0

# Imitate human amateur 9d players (roughly based on ~KGS ranks)
humanSLProfile = preaz_9d    # was preaz_5k in gtp_human5k_example.cfg
humanSLChosenMoveProp = 1.0
humanSLChosenMoveIgnorePass = true

# When a move starts to lose more than 0.06 utility (several percent winrate), downweight it.
# Increase this number to reduce the strength gain and use the human SL policy move and KataGo's evaluations less.
# Since this uses KataGo's judgment, even at large values and with a weak-ranked humanSLProfile
# this may still produce a very strong player.
# To calibrate for some but less strength gain, it will take experimentation - in addition to increasing this value a lot,
# you can also try using old/small KataGo nets (e.g. b6c96, b10c128), reducing visits (though reducing below about 30-40
# is not recommended), or using the humanSLModel itself as the main "-model".
humanSLChosenMovePiklLambda = 0.06  # was 100000000 in gtp_human5k_example.cfg.

# Spend 80% of visits to explore humanSL moves to ensure they get evaluations to use with humanSLChosenMovePiklLambda
humanSLRootExploreProbWeightless = 0.8  # was 0 in gtp_human5k_example.cfg.
humanSLRootExploreProbWeightful = 0.0
humanSLPlaExploreProbWeightless = 0.0
humanSLPlaExploreProbWeightful = 0.0
humanSLOppExploreProbWeightless = 0.0
humanSLOppExploreProbWeightful = 0.0

humanSLCpuctExploration = 0.50
# Explore moves very widely according to the human SL policy even if they lose up to about 2.0 utility.
# This is not the largest this value can go, you increase it further to do even less filtering based on winrates
# as far as move exploration goes.
humanSLCpuctPermanent = 2.0  # was 0.2 in gtp_human5k_example.cfg.

# Reduced temperature settings - reduce the chance of picking unlikely low policy or low-value moves.
chosenMoveTemperatureEarly = 0.70  # was 0.85 in gtp_human5k_example.cfg.
chosenMoveTemperature = 0.25       # was 0.70 in gtp_human5k_example.cfg.
chosenMoveTemperatureHalflife = 30         # was 80 in gtp_human5k_example.cfg.
chosenMoveTemperatureOnlyBelowProb = 1.0   # was 0.01 in gtp_human5k_example.cfg.
chosenMoveSubtract = 0
chosenMovePrune = 0

# Since we're doing more search, increase size of neural net cache a bit for performance.
nnCacheSizePowerOfTwo = 20    # was 17 in gtp_human5k_example.cfg.
nnMutexPoolSizePowerOfTwo = 14

ignorePreRootHistory = false
analysisIgnorePreRootHistory = false

rootNumSymmetriesToSample = 2
useLcbForSelection = false

# Make score utility count a bit more
winLossUtilityFactor = 1.0
staticScoreUtilityFactor = 0.5    # was 0.3 in gtp_human5k_example.cfg.
dynamicScoreUtilityFactor = 0.5   # was 0.0 in gtp_human5k_example.cfg.

useUncertainty = false
subtreeValueBiasFactor = 0.0
useNoisePruning = false
