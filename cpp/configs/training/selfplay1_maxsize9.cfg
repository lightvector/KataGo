
# Logs------------------------------------------------------------------------------------

logSearchInfo = false
logMoves = false
logGamesEvery = 10
logToStdout = true

# Data writing-----------------------------------------------------------------------------------

# Spatial size of tensors of data, must match -pos-len in python/train.py and be at least as large as the
# largest boardsize in the data. If you train only on smaller board sizes, you can decrease this and
# pass in a smaller -pos-len to python/train.py (e.g. modify "-pos-len 19" in selfplay/train.sh script),
# to train faster, although it may be awkward if you ever want to increase it again later since data with
# different values cannot be shuffled together.
dataBoardLen = 19

maxDataQueueSize = 2000
maxRowsPerTrainFile = 10000
firstFileRandMinProp = 0.15

# Fancy game selfplay settings--------------------------------------------------------------------

# These take dominance - if a game is forked for any of these reasons, that will be the next game played.
# For forked games, randomization of rules and "initGamesWithPolicy" is disabled, komi is made fair with prob "forkCompensateKomiProb".
earlyForkGameProb = 0.04  # Fork to try alternative opening variety with this probability
earlyForkGameExpectedMoveProp = 0.025  # Fork roughly within the first (boardArea * this) many moves
forkGameProb = 0.01 # Fork to try alternative crazy move anywhere in game with this probability if not early forking
forkGameMinChoices = 3   # Choose from the best of at least this many random choices
earlyForkGameMaxChoices = 12  # Choose from the best of at most this many random choices
forkGameMaxChoices = 36  # Choose from the best of at most this many random choices
# Otherwise, with this probability, learn a bit more about the differing evaluation of seki in different rulesets.
sekiForkHackProb = 0.02

# Otherwise, play some proportion of games starting from SGF positions, with randomized rules (ignoring the sgf rules)
# On SGF positions, high temperature policy init is allowed
# startPosesProb = 0.0  # Play this proportion of games starting from SGF positions
# startPosesFromSgfDir = DIRECTORYPATH  # Load SGFs from this dir
# startPosesLoadProb = 1.0  # Only load each position from each SGF with this chance (save memory)
# startPosesTurnWeightLambda = 0  # 0 = equal weight  0.01 = decrease probability by 1% per turn  -0.01 = increase probability by 1% per turn.
# startPosesPolicyInitAreaProp = 0.0  # Same as policyInitAreaProp but for SGF positions

# Otherwise, play some proportion of games starting from hint positions (generated using "dataminesgfs" command), with randomized rules.
# On hint positions, "initGamesWithPolicy" does not apply.
# hintPosesProb = 0.0
# hintPosesDir = DIRECTORYPATH

# Otherwise we are playing a "normal" game, potentially with handicap stones, depending on "handicapProb", and
# potentially with komi randomization, depending on things like "komiStdev", and potentially with different
# board sizes, etc.

# Most of the remaining parameters here below apply regardless of the initialization, although a few of them
# vary depending on handicap vs normal game, and some are explicitly disabled (e.g. initGamesWithPolicy on hint positions).

initGamesWithPolicy = true  # Play the first few moves of a game high-temperaturely from policy
policyInitAreaProp = 0.04 # The avg number of moves to play
compensateAfterPolicyInitProb = 0.2 # Additionally make komi fair this often after the high-temperature moves.
sidePositionProb = 0.020  # With this probability, train on refuting bad alternative moves.

cheapSearchProb = 0.75  # Do cheap searches with this probaiblity
cheapSearchVisits = 100  # Number of visits for cheap search
cheapSearchTargetWeight = 0.0  # Training weight for cheap search

reduceVisits = true  # Reduce visits when one side is winning
reduceVisitsThreshold = 0.9  # How winning a side needs to be (winrate)
reduceVisitsThresholdLookback = 3  # How many consecutive turns needed to be that winning
reducedVisitsMin = 100  # Minimum number of visits (never reduce below this)
reducedVisitsWeight = 0.1  # Minimum training weight

handicapAsymmetricPlayoutProb = 0.5  # In handicap games, play with unbalanced players with this probablity
normalAsymmetricPlayoutProb = 0.01  # In regular games, play with unbalanced players with this probability
maxAsymmetricRatio = 8.0 # Max ratio to unbalance visits between players
minAsymmetricCompensateKomiProb = 0.4 # Compensate komi with at least this probability for unbalanced players

policySurpriseDataWeight = 0.5  # This proportion of training weight should be concentrated on surprising moves
valueSurpriseDataWeight = 0.1   # This proportion of training weight should be concentrated on surprising position results

estimateLeadProb = 0.05 # Train lead, rather than just scoremean. Consumes a decent number of extra visits, can be quite slow using low visits to set too high.
switchNetsMidGame = true  # When a new neural net is loaded, switch to it immediately instead of waiting for new game
fancyKomiVarying = true  # In non-compensated handicap and fork games, vary komi to better learn komi and large score differences that would never happen in even games.

# Match-----------------------------------------------------------------------------------

numGameThreads = 16
maxMovesPerGame = 1600

# Rules------------------------------------------------------------------------------------

koRules = SIMPLE,POSITIONAL,SITUATIONAL
scoringRules = AREA,TERRITORY
taxRules = NONE,NONE,SEKI,SEKI,ALL
multiStoneSuicideLegals = false,true
hasButtons = false,false,true

bSizes = 7,8,9
bSizeRelProbs = 1,1,8
allowRectangleProb = 0.50

komiAuto = True  # Automatically adjust komi to what the neural nets think are fair based on the empty board, but still apply komiStdev.
# komiMean = 7.0 # Specify explicit komi
komiStdev = 1.0  # Standard deviation of random variation to komi.
komiBigStdevProb = 0.06 # Probability of applying komiBigStdev
komiBigStdev = 12.0 # Standard deviation of random big variation to komi

handicapProb = 0.10 # Probability of handicap game
handicapCompensateKomiProb = 0.50 # In handicap games, adjust komi to fair with this probability based on the handicap placement
forkCompensateKomiProb = 0.80     # For forks, adjust komi to fair with this probability based on the forked position
sgfCompensateKomiProb = 0.90      # For sgfs, adjust komi to fair with this probability based on the specific starting position

drawRandRadius = 0.5
noResultStdev = 0.166666666

# Search limits-----------------------------------------------------------------------------------

maxVisits = 600
numSearchThreads = 1

# GPU Settings-------------------------------------------------------------------------------

nnMaxBatchSize = 8
nnCacheSizePowerOfTwo = 21
nnMutexPoolSizePowerOfTwo = 15
numNNServerThreadsPerModel = 2
nnRandomize = true

coremlDeviceToUseThread0 = 0 # GPU
coremlDeviceToUseThread1 = 1 # GPU

# Root move selection and biases------------------------------------------------------------------------------

chosenMoveTemperatureEarly = 0.75
chosenMoveTemperatureHalflife = 19
chosenMoveTemperature = 0.15
chosenMoveSubtract = 0
chosenMovePrune = 1

rootNoiseEnabled = true
rootDirichletNoiseTotalConcentration = 10.83
rootDirichletNoiseWeight = 0.25

rootDesiredPerChildVisitsCoeff = 2
rootNumSymmetriesToSample = 4

useLcbForSelection = true
lcbStdevs = 5.0
minVisitPropForLCB = 0.15

# Internal params------------------------------------------------------------------------------

winLossUtilityFactor = 1.0
staticScoreUtilityFactor = 0.00
dynamicScoreUtilityFactor = 0.40
dynamicScoreCenterZeroWeight = 0.25
dynamicScoreCenterScale = 0.50
noResultUtilityForWhite = 0.0
drawEquivalentWinsForWhite = 0.5

rootEndingBonusPoints = 0.5
rootPruneUselessMoves = true

rootPolicyTemperatureEarly = 1.25
rootPolicyTemperature = 1.1

cpuctExploration = 1.1
cpuctExplorationLog = 0.0
fpuReductionMax = 0.2
rootFpuReductionMax = 0.0

numVirtualLossesPerThread = 1

# These parameters didn't exist historically during early KataGo runs
valueWeightExponent = 0.5
subtreeValueBiasFactor = 0.30
subtreeValueBiasWeightExponent = 0.8
useNonBuggyLcb = true
useGraphSearch = true
fpuParentWeightByVisitedPolicy = true
fpuParentWeightByVisitedPolicyPow = 2.0
