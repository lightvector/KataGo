#Example config for C++ (non-python) gtp bot

#Logs------------------------------------------------------------------------------------

#Where to output log?
logFile = gtp.log
#Logging options
logAllGTPCommunication = true
logSearchInfo = true

#Rules------------------------------------------------------------------------------------

#koRule = SIMPLE        #Simple ko rules (triple ko = no result)
koRule = POSITIONAL     #Positional superko
#koRule = SITUATIONAL   #Situational superko
#koRule = SPIGHT        #Spight superko - https://senseis.xmp.net/?SpightRules

scoringRule = AREA        #Area scoring
#scoringRule = TERRITORY  #Territory scoring (uses a sort of special computer-friendly territory ruleset)

multiStoneSuicideLegal = true  #Is multiple-stone suicide legal? (Single-stone suicide is always illegal).

#Make the bot capture stones that are part of pass-alive territory
#This is necessary to get correct play under tromp-taylor rules since the bot otherwise assumes (and is trained under)
#a ruleset where those stones need not be captured. It obviously should NOT be enabled if playing under territory scoring.
cleanupBeforePass = true

#Uncomment these to allow resignation. resignThreshold is on a [-1,1] scale.
#allowResignation = true
#resignThreshold = -0.98

#Search limits-----------------------------------------------------------------------------------

#If provided, limit maximum number of root visits per search to this much. (With tree reuse, visits do count earlier search)
maxVisits = 1000
#If provided, limit maximum number of new playouts per search to this much. (With tree reuse, playouts do not count earlier search)
#maxPlayouts = 1000
#If provided, cap search time at this many seconds (search will still try to follow GTP time controls)
#maxTime = 60

#Number of threads to use in search
numSearchThreads = 1
#Ponder on the opponent's turn?
ponderingEnabled = false

#Play a little faster if the opponent is passing, for friendliness
searchFactorAfterOnePass = 0.50
searchFactorAfterTwoPass = 0.25

#GPU Settings-------------------------------------------------------------------------------

#Maximum number of positions to send to GPU at once. If your GPU has enough memory, you can increase this.
nnMaxBatchSize = 16
#Cache up to 2 ** this many neural net evaluations in case of transpositions in the tree.
nnCacheSizePowerOfTwo = 18
#Size of mutex pool for nnCache is 2 ** this
nnMutexPoolSizePowerOfTwo = 14
#How many threads should there be to feed positions to the neural net?
numNNServerThreadsPerModel = 1
#Randomize board orientation when running neural net evals?
nnRandomize = true
#If provided, force usage of a specific seed for nnRandomize instead of randomizing
#nnRandSeed = abcdefg

#CUDA GPU settings--------------------------------------
#These only apply when using CUDA as the backend for inference.
#(For GTP, we only ever have one model, when playing matches, we might have more than one, see match_example.cfg)

#Default behavior is just to always use gpu 0, you will want to uncomment and adjust one or more of these lines
#to take advantage of a multi-gpu machine
#cudaGpuToUse = 0 #use gpu 0 for all server threads (numNNServerThreadsPerModel) unless otherwise specified per-model or per-thread-per-model
#cudaGpuToUseModel0 = 3 #use gpu 3 for model 0 for all threads unless otherwise specified per-thread for this model
#cudaGpuToUseModel1 = 2 #use gpu 2 for model 1 for all threads unless otherwise specified per-thread for this model
#cudaGpuToUseModel0Thread0 = 3 #use gpu 3 for model 0, server thread 0
#cudaGpuToUseModel0Thread1 = 2 #use gpu 2 for model 0, server thread 1

#Uncomment these on NVIDIA devices with FP16 tensor cores for probably a speedup, at the cost of introducing some precision loss in the nn calculation.
#cudaUseFP16 = true
#cudaUseNHWC = true

#Search randomization------------------------------------------------------------------------------
#Note that multithreading can also introduce a significant amount of nondeterminism.

#If provided, force usage of a specific seed for various things in the search instead of randomizing
#searchRandSeed = hijklmn

#Temperature for the early game, randomize between chosen moves with this temperature
chosenMoveTemperatureEarly = 0.5
#Decay temperature for the early game by 0.5 every this many moves, scaled with board size.
chosenMoveTemperatureHalflife = 19
#At the end of search after the early game, randomize between chosen moves with this temperature
chosenMoveTemperature = 0.20
#Subtract this many visits from each move prior to applying chosenMoveTemperature
#(unless all moves have too few visits) to downweight unlikely moves
chosenMoveSubtract = 0
#The same as chosenMoveSubtract but only prunes moves that fall below the threshold, does not affect moves above
chosenMovePrune = 1

#Use dirichlet noise for the root node policy?
rootNoiseEnabled = false
#Dirichlet noise alpha is set to this divided by number of legal moves. 10.83 produces an alpha of 0.03 on an empty 19x19 board.
rootDirichletNoiseTotalConcentration = 10.83
#Proportion of root policy that is noise
rootDirichletNoiseWeight = 0.25


#Internal params------------------------------------------------------------------------------

#Scales the utility of winning/losing
winLossUtilityFactor = 1.0
#Scales the utility for trying to maximize score
staticScoreUtilityFactor = 0.30
dynamicScoreUtilityFactor = 0.00
#The utility of getting a "no result" due to triple ko or other long cycle in non-superko rulesets (-1 to 1)
noResultUtilityForWhite = 0.0
#The number of wins that a draw counts as, for white. (0 to 1)
drawEquivalentWinsForWhite = 0.5

#Exploration constant for mcts
cpuctExploration = 1.0
#FPU reduction constant for mcts
fpuReductionMax = 0.2
#Use parent average value for fpu base point instead of point value net estimate
fpuUseParentAverage = true
#Amount to apply a downweighting of children with very bad values relative to good ones
valueWeightExponent = 0.5
#Slight incentive for the bot to behave human-like with regard to passing at the end, filling the dame,
#not wasting time playing in its own territory, etc, and not play moves that are equivalent in terms of
#points but a bit more unfriendly to humans.
rootEndingBonusPoints = 0.5
#Make the bot prune useless moves that are just prolonging the game to avoid losing yet
rootPruneUselessSuicides = true

#How big to make the mutex pool for search synchronization
mutexPoolSize = 8192
#How many virtual losses to add when a thread descends through a node
numVirtualLossesPerThread = 3




